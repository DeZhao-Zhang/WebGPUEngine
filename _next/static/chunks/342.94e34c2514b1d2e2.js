(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[342],{5671:function(e,n,t){"use strict";t.d(n,{T:function(){return d}});var r=t(5893),a=t(9008),i=t.n(a),o=t(1163),s=t(7294),u=t(9147),c=t.n(u);t(7319);let l=e=>{let n=(0,s.useRef)(null),a=(0,s.useMemo)(()=>e.sources.map(e=>{let{name:n,contents:a}=e;return{name:n,...function(e){let n;let a=null;{a=document.createElement("div");let i=t(4631);n=i(a,{lineNumbers:!0,lineWrapping:!0,theme:"monokai",readOnly:!0})}return{Container:function(t){return(0,r.jsx)("div",{...t,children:(0,r.jsx)("div",{ref(t){a&&t&&(t.appendChild(a),n.setOption("value",e))}})})}}}(a)}}),e.sources),u=(0,s.useRef)(null),l=(0,s.useMemo)(()=>{if(e.gui){let n=t(4376);return new n.GUI({autoPlace:!1})}},[]),d=(0,o.useRouter)(),f=d.asPath.match(/#([a-zA-Z0-9\.\/]+)/),[p,m]=(0,s.useState)(null),[h,g]=(0,s.useState)(null);return(0,s.useEffect)(()=>{f?g(f[1]):g(a[0].name),l&&u.current&&u.current.appendChild(l.domElement);let t={active:!0},r=()=>{t.active=!1};try{let i=n.current,o=e.init({canvas:i,pageState:t,gui:l});o instanceof Promise&&o.catch(e=>{console.error(e),m(e)})}catch(s){console.error(s),m(s)}return r},[]),(0,r.jsxs)("main",{children:[(0,r.jsxs)(i(),{children:[(0,r.jsx)("style",{dangerouslySetInnerHTML:{__html:"\n            .CodeMirror {\n              height: auto !important;\n              margin: 1em 0;\n            }\n\n            .CodeMirror-scroll {\n              height: auto !important;\n              overflow: visible !important;\n            }\n          "}}),(0,r.jsx)("title",{children:"".concat(e.name," - WebGPU Samples")}),(0,r.jsx)("meta",{name:"description",content:e.description}),(0,r.jsx)("meta",{httpEquiv:"origin-trial",content:e.originTrial})]}),(0,r.jsxs)("div",{children:[(0,r.jsx)("h1",{children:e.name}),(0,r.jsx)("a",{target:"_blank",rel:"noreferrer",href:"https://github.com/".concat("webgpu/webgpu-samples","/tree/main/").concat(e.filename),children:"See it on Github!"}),(0,r.jsx)("p",{children:e.description}),p?(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)("p",{children:"Is WebGPU Enabled?"}),(0,r.jsx)("p",{children:"".concat(p)})]}):null]}),(0,r.jsxs)("div",{className:c().canvasContainer,children:[(0,r.jsx)("div",{style:{position:"absolute",right:10},ref:u}),(0,r.jsx)("canvas",{ref:n})]}),(0,r.jsxs)("div",{children:[(0,r.jsx)("nav",{className:c().sourceFileNav,children:(0,r.jsx)("ul",{children:a.map((e,n)=>(0,r.jsx)("li",{children:(0,r.jsx)("a",{href:"#".concat(e.name),"data-active":h==e.name,onClick(){g(e.name)},children:e.name})},n))})}),a.map((e,n)=>(0,r.jsx)(e.Container,{className:c().sourceFileContainer,"data-active":h==e.name},n))]})]})},d=e=>(0,r.jsx)(l,{...e})},3057:function(e,n,t){"use strict";t.d(n,{W:function(){return o}});var r=t(6906),a=t(3560);let i={xy:[0,1],xz:[0,2],yz:[1,2]},o={positions:r.m,triangles:r.g,normals:[],uvs:[]};o.normals=function(e,n){let t=e.map(()=>[0,0,0]);return n.forEach(n=>{let[r,i,o]=n,s=e[r],u=e[i],c=e[o],l=a.R3.subtract(u,s),d=a.R3.subtract(c,s);a.R3.normalize(l,l),a.R3.normalize(d,d);let f=a.R3.cross(l,d);a.R3.add(t[r],f,t[r]),a.R3.add(t[i],f,t[i]),a.R3.add(t[o],f,t[o])}),t.forEach(e=>{a.R3.normalize(e,e)}),t}(o.positions,o.triangles),o.uvs=function(e){let n=arguments.length>1&&void 0!==arguments[1]?arguments[1]:"xy",t=i[n],r=e.map(()=>[0,0]),a=[1/0,1/0],o=[-1/0,-1/0];return e.forEach((e,n)=>{r[n][0]=e[t[0]],r[n][1]=e[t[1]],a[0]=Math.min(e[t[0]],a[0]),a[1]=Math.min(e[t[1]],a[1]),o[0]=Math.max(e[t[0]],o[0]),o[1]=Math.max(e[t[1]],o[1])}),r.forEach(e=>{e[0]=(e[0]-a[0])/(o[0]-a[0]),e[1]=(e[1]-a[1])/(o[1]-a[1])}),r}(o.positions,"xy"),o.triangles.push([o.positions.length,o.positions.length+2,o.positions.length+1],[o.positions.length,o.positions.length+1,o.positions.length+3]),o.positions.push([-100,20,-100],[100,20,100],[-100,20,100],[100,20,-100]),o.normals.push([0,1,0],[0,1,0],[0,1,0],[0,1,0]),o.uvs.push([0,0],[1,1],[0,1],[1,0])},2342:function(e,n,t){"use strict";t.r(n),t.d(n,{default:function(){return f}});var r=t(3560),a=t(5671),i=t(3057),o="struct Scene {\n  lightViewProjMatrix: mat4x4<f32>,\n  cameraViewProjMatrix: mat4x4<f32>,\n  lightPos: vec3<f32>,\n}\n\nstruct Model {\n  modelMatrix: mat4x4<f32>,\n}\n\n@group(0) @binding(0) var<uniform> scene : Scene;\n@group(1) @binding(0) var<uniform> model : Model;\n\n@vertex\nfn main(\n  @location(0) position: vec3<f32>\n) -> @builtin(position) vec4<f32> {\n  return scene.lightViewProjMatrix * model.modelMatrix * vec4(position, 1.0);\n}\n",s="struct Scene {\n  lightViewProjMatrix: mat4x4<f32>,\n  cameraViewProjMatrix: mat4x4<f32>,\n  lightPos: vec3<f32>,\n}\n\nstruct Model {\n  modelMatrix: mat4x4<f32>,\n}\n\n@group(0) @binding(0) var<uniform> scene : Scene;\n@group(1) @binding(0) var<uniform> model : Model;\n\nstruct VertexOutput {\n  @location(0) shadowPos: vec3<f32>,\n  @location(1) fragPos: vec3<f32>,\n  @location(2) fragNorm: vec3<f32>,\n\n  @builtin(position) Position: vec4<f32>,\n}\n\n@vertex\nfn main(\n  @location(0) position: vec3<f32>,\n  @location(1) normal: vec3<f32>\n) -> VertexOutput {\n  var output : VertexOutput;\n\n  // XY is in (-1, 1) space, Z is in (0, 1) space\n  let posFromLight = scene.lightViewProjMatrix * model.modelMatrix * vec4(position, 1.0);\n\n  // Convert XY to (0, 1)\n  // Y is flipped because texture coords are Y-down.\n  output.shadowPos = vec3(\n    posFromLight.xy * vec2(0.5, -0.5) + vec2(0.5),\n    posFromLight.z\n  );\n\n  output.Position = scene.cameraViewProjMatrix * model.modelMatrix * vec4(position, 1.0);\n  output.fragPos = output.Position.xyz;\n  output.fragNorm = normal;\n  return output;\n}\n",u="override shadowDepthTextureSize: f32 = 1024.0;\n\nstruct Scene {\n  lightViewProjMatrix : mat4x4<f32>,\n  cameraViewProjMatrix : mat4x4<f32>,\n  lightPos : vec3<f32>,\n}\n\n@group(0) @binding(0) var<uniform> scene : Scene;\n@group(0) @binding(1) var shadowMap: texture_depth_2d;\n@group(0) @binding(2) var shadowSampler: sampler_comparison;\n\nstruct FragmentInput {\n  @location(0) shadowPos : vec3<f32>,\n  @location(1) fragPos : vec3<f32>,\n  @location(2) fragNorm : vec3<f32>,\n}\n\nconst albedo = vec3<f32>(0.9);\nconst ambientFactor = 0.2;\n\n@fragment\nfn main(input : FragmentInput) -> @location(0) vec4<f32> {\n  // Percentage-closer filtering. Sample texels in the region\n  // to smooth the result.\n  var visibility = 0.0;\n  let oneOverShadowDepthTextureSize = 1.0 / shadowDepthTextureSize;\n  for (var y = -1; y <= 1; y++) {\n    for (var x = -1; x <= 1; x++) {\n      let offset = vec2<f32>(vec2(x, y)) * oneOverShadowDepthTextureSize;\n\n      visibility += textureSampleCompare(\n        shadowMap, shadowSampler,\n        input.shadowPos.xy + offset, input.shadowPos.z - 0.007\n      );\n    }\n  }\n  visibility /= 9.0;\n\n  let lambertFactor = max(dot(normalize(scene.lightPos - input.fragPos), input.fragNorm), 0.0);\n  let lightingFactor = min(ambientFactor + visibility * lambertFactor, 1.0);\n\n  return vec4(lightingFactor * albedo, 1.0);\n}\n",c="src/sample/shadowMapping/main.ts";let l=async e=>{let{canvas:n,pageState:t}=e,a=await navigator.gpu.requestAdapter(),c=await a.requestDevice();if(!t.active)return;let l=n.getContext("webgpu"),d=window.devicePixelRatio||1;n.width=n.clientWidth*d,n.height=n.clientHeight*d;let f=n.width/n.height,p=navigator.gpu.getPreferredCanvasFormat();l.configure({device:c,format:p,alphaMode:"premultiplied"});let m=c.createBuffer({size:6*i.W.positions.length*Float32Array.BYTES_PER_ELEMENT,usage:GPUBufferUsage.VERTEX,mappedAtCreation:!0});{let h=new Float32Array(m.getMappedRange());for(let g=0;g<i.W.positions.length;++g)h.set(i.W.positions[g],6*g),h.set(i.W.normals[g],6*g+3);m.unmap()}let v=3*i.W.triangles.length,x=c.createBuffer({size:v*Uint16Array.BYTES_PER_ELEMENT,usage:GPUBufferUsage.INDEX,mappedAtCreation:!0});{let P=new Uint16Array(x.getMappedRange());for(let w=0;w<i.W.triangles.length;++w)P.set(i.W.triangles[w],3*w);x.unmap()}let b=c.createTexture({size:[1024,1024,1],usage:GPUTextureUsage.RENDER_ATTACHMENT|GPUTextureUsage.TEXTURE_BINDING,format:"depth32float"}),y=b.createView(),S=[{arrayStride:6*Float32Array.BYTES_PER_ELEMENT,attributes:[{shaderLocation:0,offset:0,format:"float32x3"},{shaderLocation:1,offset:3*Float32Array.BYTES_PER_ELEMENT,format:"float32x3"}]}],E={topology:"triangle-list",cullMode:"back"},B=c.createBindGroupLayout({entries:[{binding:0,visibility:GPUShaderStage.VERTEX,buffer:{type:"uniform"}}]}),M=c.createRenderPipeline({layout:c.createPipelineLayout({bindGroupLayouts:[B,B]}),vertex:{module:c.createShaderModule({code:o}),entryPoint:"main",buffers:S},depthStencil:{depthWriteEnabled:!0,depthCompare:"less",format:"depth32float"},primitive:E}),G=c.createBindGroupLayout({entries:[{binding:0,visibility:GPUShaderStage.VERTEX|GPUShaderStage.FRAGMENT,buffer:{type:"uniform"}},{binding:1,visibility:GPUShaderStage.VERTEX|GPUShaderStage.FRAGMENT,texture:{sampleType:"depth"}},{binding:2,visibility:GPUShaderStage.VERTEX|GPUShaderStage.FRAGMENT,sampler:{type:"comparison"}}]}),T=c.createRenderPipeline({layout:c.createPipelineLayout({bindGroupLayouts:[G,B]}),vertex:{module:c.createShaderModule({code:s}),entryPoint:"main",buffers:S},fragment:{module:c.createShaderModule({code:u}),entryPoint:"main",targets:[{format:p}],constants:{shadowDepthTextureSize:1024}},depthStencil:{depthWriteEnabled:!0,depthCompare:"less",format:"depth24plus-stencil8"},primitive:E}),R=c.createTexture({size:[n.width,n.height],format:"depth24plus-stencil8",usage:GPUTextureUsage.RENDER_ATTACHMENT}),U={colorAttachments:[{view:void 0,clearValue:{r:.5,g:.5,b:.5,a:1},loadOp:"clear",storeOp:"store"}],depthStencilAttachment:{view:R.createView(),depthClearValue:1,depthLoadOp:"clear",depthStoreOp:"store",stencilClearValue:0,stencilLoadOp:"clear",stencilStoreOp:"store"}},_=c.createBuffer({size:64,usage:GPUBufferUsage.UNIFORM|GPUBufferUsage.COPY_DST}),V=c.createBuffer({size:144,usage:GPUBufferUsage.UNIFORM|GPUBufferUsage.COPY_DST}),L=c.createBindGroup({layout:B,entries:[{binding:0,resource:{buffer:V}}]}),C=c.createBindGroup({layout:G,entries:[{binding:0,resource:{buffer:V}},{binding:1,resource:y},{binding:2,resource:c.createSampler({compare:"less"})}]}),A=c.createBindGroup({layout:B,entries:[{binding:0,resource:{buffer:_}}]}),F=r.R3.fromValues(0,50,-100),D=r.R3.fromValues(0,1,0),j=r.R3.fromValues(0,0,0),N=r._E.perspective(2*Math.PI/5,f,1,2e3),O=r._E.inverse(r._E.lookAt(F,j,D)),z=r.R3.fromValues(50,100,-100),I=r._E.inverse(r._E.lookAt(z,j,D)),W=r._E.create();r._E.ortho(-80,80,-80,80,-200,300,W);let q=r._E.multiply(W,I),Y=r._E.multiply(N,O),k=r._E.translation([0,-45,0]);c.queue.writeBuffer(V,0,q.buffer,q.byteOffset,q.byteLength),c.queue.writeBuffer(V,64,Y.buffer,Y.byteOffset,Y.byteLength),c.queue.writeBuffer(V,128,z.buffer,z.byteOffset,z.byteLength),c.queue.writeBuffer(_,0,k.buffer,k.byteOffset,k.byteLength);let X={colorAttachments:[],depthStencilAttachment:{view:y,depthClearValue:1,depthLoadOp:"clear",depthStoreOp:"store"}};requestAnimationFrame(function e(){if(!t.active)return;let n=function(){let e=r.R3.fromValues(0,50,-100),n=Math.PI*(Date.now()/2e3),t=r._E.rotateY(r._E.translation(j),n);r.R3.transformMat4(e,t,e);let a=r._E.inverse(r._E.lookAt(e,j,D));return r._E.multiply(N,a,Y),Y}();c.queue.writeBuffer(V,64,n.buffer,n.byteOffset,n.byteLength),U.colorAttachments[0].view=l.getCurrentTexture().createView();let a=c.createCommandEncoder();{let i=a.beginRenderPass(X);i.setPipeline(M),i.setBindGroup(0,L),i.setBindGroup(1,A),i.setVertexBuffer(0,m),i.setIndexBuffer(x,"uint16"),i.drawIndexed(v),i.end()}{let o=a.beginRenderPass(U);o.setPipeline(T),o.setBindGroup(0,C),o.setBindGroup(1,A),o.setVertexBuffer(0,m),o.setIndexBuffer(x,"uint16"),o.drawIndexed(v),o.end()}c.queue.submit([a.finish()]),requestAnimationFrame(e)})},d=()=>(0,a.T)({name:"Shadow Mapping",description:"This example shows how to sample from a depth texture to render shadows.",init:l,sources:[{name:c.substring(25),contents:"import { mat4, vec3 } from 'wgpu-matrix';\nimport { makeSample, SampleInit } from '../../components/SampleLayout';\n\nimport { mesh } from '../../meshes/stanfordDragon';\n\nimport vertexShadowWGSL from './vertexShadow.wgsl';\nimport vertexWGSL from './vertex.wgsl';\nimport fragmentWGSL from './fragment.wgsl';\n\nconst shadowDepthTextureSize = 1024;\n\nconst init: SampleInit = async ({ canvas, pageState }) => {\n  const adapter = await navigator.gpu.requestAdapter();\n  const device = await adapter.requestDevice();\n\n  if (!pageState.active) return;\n\n  const context = canvas.getContext('webgpu') as GPUCanvasContext;\n\n  const devicePixelRatio = window.devicePixelRatio || 1;\n  canvas.width = canvas.clientWidth * devicePixelRatio;\n  canvas.height = canvas.clientHeight * devicePixelRatio;\n  const aspect = canvas.width / canvas.height;\n  const presentationFormat = navigator.gpu.getPreferredCanvasFormat();\n  context.configure({\n    device,\n    format: presentationFormat,\n    alphaMode: 'premultiplied',\n  });\n\n  // Create the model vertex buffer.\n  const vertexBuffer = device.createBuffer({\n    size: mesh.positions.length * 3 * 2 * Float32Array.BYTES_PER_ELEMENT,\n    usage: GPUBufferUsage.VERTEX,\n    mappedAtCreation: true,\n  });\n  {\n    const mapping = new Float32Array(vertexBuffer.getMappedRange());\n    for (let i = 0; i < mesh.positions.length; ++i) {\n      mapping.set(mesh.positions[i], 6 * i);\n      mapping.set(mesh.normals[i], 6 * i + 3);\n    }\n    vertexBuffer.unmap();\n  }\n\n  // Create the model index buffer.\n  const indexCount = mesh.triangles.length * 3;\n  const indexBuffer = device.createBuffer({\n    size: indexCount * Uint16Array.BYTES_PER_ELEMENT,\n    usage: GPUBufferUsage.INDEX,\n    mappedAtCreation: true,\n  });\n  {\n    const mapping = new Uint16Array(indexBuffer.getMappedRange());\n    for (let i = 0; i < mesh.triangles.length; ++i) {\n      mapping.set(mesh.triangles[i], 3 * i);\n    }\n    indexBuffer.unmap();\n  }\n\n  // Create the depth texture for rendering/sampling the shadow map.\n  const shadowDepthTexture = device.createTexture({\n    size: [shadowDepthTextureSize, shadowDepthTextureSize, 1],\n    usage: GPUTextureUsage.RENDER_ATTACHMENT | GPUTextureUsage.TEXTURE_BINDING,\n    format: 'depth32float',\n  });\n  const shadowDepthTextureView = shadowDepthTexture.createView();\n\n  // Create some common descriptors used for both the shadow pipeline\n  // and the color rendering pipeline.\n  const vertexBuffers: Iterable<GPUVertexBufferLayout> = [\n    {\n      arrayStride: Float32Array.BYTES_PER_ELEMENT * 6,\n      attributes: [\n        {\n          // position\n          shaderLocation: 0,\n          offset: 0,\n          format: 'float32x3',\n        },\n        {\n          // normal\n          shaderLocation: 1,\n          offset: Float32Array.BYTES_PER_ELEMENT * 3,\n          format: 'float32x3',\n        },\n      ],\n    },\n  ];\n\n  const primitive: GPUPrimitiveState = {\n    topology: 'triangle-list',\n    cullMode: 'back',\n  };\n\n  const uniformBufferBindGroupLayout = device.createBindGroupLayout({\n    entries: [\n      {\n        binding: 0,\n        visibility: GPUShaderStage.VERTEX,\n        buffer: {\n          type: 'uniform',\n        },\n      },\n    ],\n  });\n\n  const shadowPipeline = device.createRenderPipeline({\n    layout: device.createPipelineLayout({\n      bindGroupLayouts: [\n        uniformBufferBindGroupLayout,\n        uniformBufferBindGroupLayout,\n      ],\n    }),\n    vertex: {\n      module: device.createShaderModule({\n        code: vertexShadowWGSL,\n      }),\n      entryPoint: 'main',\n      buffers: vertexBuffers,\n    },\n    depthStencil: {\n      depthWriteEnabled: true,\n      depthCompare: 'less',\n      format: 'depth32float',\n    },\n    primitive,\n  });\n\n  // Create a bind group layout which holds the scene uniforms and\n  // the texture+sampler for depth. We create it manually because the WebPU\n  // implementation doesn't infer this from the shader (yet).\n  const bglForRender = device.createBindGroupLayout({\n    entries: [\n      {\n        binding: 0,\n        visibility: GPUShaderStage.VERTEX | GPUShaderStage.FRAGMENT,\n        buffer: {\n          type: 'uniform',\n        },\n      },\n      {\n        binding: 1,\n        visibility: GPUShaderStage.VERTEX | GPUShaderStage.FRAGMENT,\n        texture: {\n          sampleType: 'depth',\n        },\n      },\n      {\n        binding: 2,\n        visibility: GPUShaderStage.VERTEX | GPUShaderStage.FRAGMENT,\n        sampler: {\n          type: 'comparison',\n        },\n      },\n    ],\n  });\n\n  const pipeline = device.createRenderPipeline({\n    layout: device.createPipelineLayout({\n      bindGroupLayouts: [bglForRender, uniformBufferBindGroupLayout],\n    }),\n    vertex: {\n      module: device.createShaderModule({\n        code: vertexWGSL,\n      }),\n      entryPoint: 'main',\n      buffers: vertexBuffers,\n    },\n    fragment: {\n      module: device.createShaderModule({\n        code: fragmentWGSL,\n      }),\n      entryPoint: 'main',\n      targets: [\n        {\n          format: presentationFormat,\n        },\n      ],\n      constants: {\n        shadowDepthTextureSize,\n      },\n    },\n    depthStencil: {\n      depthWriteEnabled: true,\n      depthCompare: 'less',\n      format: 'depth24plus-stencil8',\n    },\n    primitive,\n  });\n\n  const depthTexture = device.createTexture({\n    size: [canvas.width, canvas.height],\n    format: 'depth24plus-stencil8',\n    usage: GPUTextureUsage.RENDER_ATTACHMENT,\n  });\n\n  const renderPassDescriptor: GPURenderPassDescriptor = {\n    colorAttachments: [\n      {\n        // view is acquired and set in render loop.\n        view: undefined,\n\n        clearValue: { r: 0.5, g: 0.5, b: 0.5, a: 1.0 },\n        loadOp: 'clear',\n        storeOp: 'store',\n      },\n    ],\n    depthStencilAttachment: {\n      view: depthTexture.createView(),\n\n      depthClearValue: 1.0,\n      depthLoadOp: 'clear',\n      depthStoreOp: 'store',\n      stencilClearValue: 0,\n      stencilLoadOp: 'clear',\n      stencilStoreOp: 'store',\n    },\n  };\n\n  const modelUniformBuffer = device.createBuffer({\n    size: 4 * 16, // 4x4 matrix\n    usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,\n  });\n\n  const sceneUniformBuffer = device.createBuffer({\n    // Two 4x4 viewProj matrices,\n    // one for the camera and one for the light.\n    // Then a vec3 for the light position.\n    // Rounded to the nearest multiple of 16.\n    size: 2 * 4 * 16 + 4 * 4,\n    usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,\n  });\n\n  const sceneBindGroupForShadow = device.createBindGroup({\n    layout: uniformBufferBindGroupLayout,\n    entries: [\n      {\n        binding: 0,\n        resource: {\n          buffer: sceneUniformBuffer,\n        },\n      },\n    ],\n  });\n\n  const sceneBindGroupForRender = device.createBindGroup({\n    layout: bglForRender,\n    entries: [\n      {\n        binding: 0,\n        resource: {\n          buffer: sceneUniformBuffer,\n        },\n      },\n      {\n        binding: 1,\n        resource: shadowDepthTextureView,\n      },\n      {\n        binding: 2,\n        resource: device.createSampler({\n          compare: 'less',\n        }),\n      },\n    ],\n  });\n\n  const modelBindGroup = device.createBindGroup({\n    layout: uniformBufferBindGroupLayout,\n    entries: [\n      {\n        binding: 0,\n        resource: {\n          buffer: modelUniformBuffer,\n        },\n      },\n    ],\n  });\n\n  const eyePosition = vec3.fromValues(0, 50, -100);\n  const upVector = vec3.fromValues(0, 1, 0);\n  const origin = vec3.fromValues(0, 0, 0);\n\n  const projectionMatrix = mat4.perspective(\n    (2 * Math.PI) / 5,\n    aspect,\n    1,\n    2000.0\n  );\n\n  const viewMatrix = mat4.inverse(mat4.lookAt(eyePosition, origin, upVector));\n\n  const lightPosition = vec3.fromValues(50, 100, -100);\n  const lightViewMatrix = mat4.inverse(\n    mat4.lookAt(lightPosition, origin, upVector)\n  );\n\n  const lightProjectionMatrix = mat4.create();\n  {\n    const left = -80;\n    const right = 80;\n    const bottom = -80;\n    const top = 80;\n    const near = -200;\n    const far = 300;\n    mat4.ortho(left, right, bottom, top, near, far, lightProjectionMatrix);\n  }\n\n  const lightViewProjMatrix = mat4.multiply(\n    lightProjectionMatrix,\n    lightViewMatrix\n  );\n\n  const viewProjMatrix = mat4.multiply(projectionMatrix, viewMatrix);\n\n  // Move the model so it's centered.\n  const modelMatrix = mat4.translation([0, -45, 0]);\n\n  // The camera/light aren't moving, so write them into buffers now.\n  {\n    const lightMatrixData = lightViewProjMatrix as Float32Array;\n    device.queue.writeBuffer(\n      sceneUniformBuffer,\n      0,\n      lightMatrixData.buffer,\n      lightMatrixData.byteOffset,\n      lightMatrixData.byteLength\n    );\n\n    const cameraMatrixData = viewProjMatrix as Float32Array;\n    device.queue.writeBuffer(\n      sceneUniformBuffer,\n      64,\n      cameraMatrixData.buffer,\n      cameraMatrixData.byteOffset,\n      cameraMatrixData.byteLength\n    );\n\n    const lightData = lightPosition as Float32Array;\n    device.queue.writeBuffer(\n      sceneUniformBuffer,\n      128,\n      lightData.buffer,\n      lightData.byteOffset,\n      lightData.byteLength\n    );\n\n    const modelData = modelMatrix as Float32Array;\n    device.queue.writeBuffer(\n      modelUniformBuffer,\n      0,\n      modelData.buffer,\n      modelData.byteOffset,\n      modelData.byteLength\n    );\n  }\n\n  // Rotates the camera around the origin based on time.\n  function getCameraViewProjMatrix() {\n    const eyePosition = vec3.fromValues(0, 50, -100);\n\n    const rad = Math.PI * (Date.now() / 2000);\n    const rotation = mat4.rotateY(mat4.translation(origin), rad);\n    vec3.transformMat4(eyePosition, rotation, eyePosition);\n\n    const viewMatrix = mat4.inverse(mat4.lookAt(eyePosition, origin, upVector));\n\n    mat4.multiply(projectionMatrix, viewMatrix, viewProjMatrix);\n    return viewProjMatrix as Float32Array;\n  }\n\n  const shadowPassDescriptor: GPURenderPassDescriptor = {\n    colorAttachments: [],\n    depthStencilAttachment: {\n      view: shadowDepthTextureView,\n      depthClearValue: 1.0,\n      depthLoadOp: 'clear',\n      depthStoreOp: 'store',\n    },\n  };\n\n  function frame() {\n    // Sample is no longer the active page.\n    if (!pageState.active) return;\n\n    const cameraViewProj = getCameraViewProjMatrix();\n    device.queue.writeBuffer(\n      sceneUniformBuffer,\n      64,\n      cameraViewProj.buffer,\n      cameraViewProj.byteOffset,\n      cameraViewProj.byteLength\n    );\n\n    renderPassDescriptor.colorAttachments[0].view = context\n      .getCurrentTexture()\n      .createView();\n\n    const commandEncoder = device.createCommandEncoder();\n    {\n      const shadowPass = commandEncoder.beginRenderPass(shadowPassDescriptor);\n      shadowPass.setPipeline(shadowPipeline);\n      shadowPass.setBindGroup(0, sceneBindGroupForShadow);\n      shadowPass.setBindGroup(1, modelBindGroup);\n      shadowPass.setVertexBuffer(0, vertexBuffer);\n      shadowPass.setIndexBuffer(indexBuffer, 'uint16');\n      shadowPass.drawIndexed(indexCount);\n\n      shadowPass.end();\n    }\n    {\n      const renderPass = commandEncoder.beginRenderPass(renderPassDescriptor);\n      renderPass.setPipeline(pipeline);\n      renderPass.setBindGroup(0, sceneBindGroupForRender);\n      renderPass.setBindGroup(1, modelBindGroup);\n      renderPass.setVertexBuffer(0, vertexBuffer);\n      renderPass.setIndexBuffer(indexBuffer, 'uint16');\n      renderPass.drawIndexed(indexCount);\n\n      renderPass.end();\n    }\n    device.queue.submit([commandEncoder.finish()]);\n    requestAnimationFrame(frame);\n  }\n  requestAnimationFrame(frame);\n};\n\nconst ShadowMapping: () => JSX.Element = () =>\n  makeSample({\n    name: 'Shadow Mapping',\n    description:\n      'This example shows how to sample from a depth texture to render shadows.',\n    init,\n    sources: [\n      {\n        name: __filename.substring(__dirname.length + 1),\n        contents: __SOURCE__,\n      },\n      {\n        name: './vertexShadow.wgsl',\n        contents: vertexShadowWGSL,\n        editable: true,\n      },\n      {\n        name: './vertex.wgsl',\n        contents: vertexWGSL,\n        editable: true,\n      },\n      {\n        name: './fragment.wgsl',\n        contents: fragmentWGSL,\n        editable: true,\n      },\n    ],\n    filename: __filename,\n  });\n\nexport default ShadowMapping;\n"},{name:"./vertexShadow.wgsl",contents:o,editable:!0},{name:"./vertex.wgsl",contents:s,editable:!0},{name:"./fragment.wgsl",contents:u,editable:!0}],filename:c});var f=d},9147:function(e){e.exports={canvasContainer:"SampleLayout_canvasContainer__zRR_l",sourceFileNav:"SampleLayout_sourceFileNav__ml48P",sourceFileContainer:"SampleLayout_sourceFileContainer__3s84x"}}}]);